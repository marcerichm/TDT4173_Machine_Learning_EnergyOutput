{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import pyarrow.parquet as pq \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./A/X_train_observed.parquet']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_file_path(filename, starting_directory='.'):\n",
    "    paths = []\n",
    "    for root, directories, files in os.walk(starting_directory):\n",
    "        for file in files:\n",
    "            if file == filename:\n",
    "                paths.append(os.path.join(root, file))\n",
    "                return paths\n",
    "    raise FileNotFoundError(f\"Could not find file {filename} in directory {starting_directory}\")\n",
    "get_file_path('X_train_observed.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema for file A:\n",
      "<pyarrow._parquet.ParquetSchema object at 0x1460cdb40>\n",
      "required group field_id=-1 schema {\n",
      "  optional int64 field_id=-1 date_forecast (Timestamp(isAdjustedToUTC=false, timeUnit=microseconds, is_from_converted_type=false, force_set_converted_type=false));\n",
      "  optional float field_id=-1 absolute_humidity_2m:gm3;\n",
      "  optional float field_id=-1 air_density_2m:kgm3;\n",
      "  optional float field_id=-1 ceiling_height_agl:m;\n",
      "  optional float field_id=-1 clear_sky_energy_1h:J;\n",
      "  optional float field_id=-1 clear_sky_rad:W;\n",
      "  optional float field_id=-1 cloud_base_agl:m;\n",
      "  optional float field_id=-1 dew_or_rime:idx;\n",
      "  optional float field_id=-1 dew_point_2m:K;\n",
      "  optional float field_id=-1 diffuse_rad:W;\n",
      "  optional float field_id=-1 diffuse_rad_1h:J;\n",
      "  optional float field_id=-1 direct_rad:W;\n",
      "  optional float field_id=-1 direct_rad_1h:J;\n",
      "  optional float field_id=-1 effective_cloud_cover:p;\n",
      "  optional float field_id=-1 elevation:m;\n",
      "  optional float field_id=-1 fresh_snow_12h:cm;\n",
      "  optional float field_id=-1 fresh_snow_1h:cm;\n",
      "  optional float field_id=-1 fresh_snow_24h:cm;\n",
      "  optional float field_id=-1 fresh_snow_3h:cm;\n",
      "  optional float field_id=-1 fresh_snow_6h:cm;\n",
      "  optional float field_id=-1 is_day:idx;\n",
      "  optional float field_id=-1 is_in_shadow:idx;\n",
      "  optional float field_id=-1 msl_pressure:hPa;\n",
      "  optional float field_id=-1 precip_5min:mm;\n",
      "  optional float field_id=-1 precip_type_5min:idx;\n",
      "  optional float field_id=-1 pressure_100m:hPa;\n",
      "  optional float field_id=-1 pressure_50m:hPa;\n",
      "  optional float field_id=-1 prob_rime:p;\n",
      "  optional float field_id=-1 rain_water:kgm2;\n",
      "  optional float field_id=-1 relative_humidity_1000hPa:p;\n",
      "  optional float field_id=-1 sfc_pressure:hPa;\n",
      "  optional float field_id=-1 snow_density:kgm3;\n",
      "  optional float field_id=-1 snow_depth:cm;\n",
      "  optional float field_id=-1 snow_drift:idx;\n",
      "  optional float field_id=-1 snow_melt_10min:mm;\n",
      "  optional float field_id=-1 snow_water:kgm2;\n",
      "  optional float field_id=-1 sun_azimuth:d;\n",
      "  optional float field_id=-1 sun_elevation:d;\n",
      "  optional float field_id=-1 super_cooled_liquid_water:kgm2;\n",
      "  optional float field_id=-1 t_1000hPa:K;\n",
      "  optional float field_id=-1 total_cloud_cover:p;\n",
      "  optional float field_id=-1 visibility:m;\n",
      "  optional float field_id=-1 wind_speed_10m:ms;\n",
      "  optional float field_id=-1 wind_speed_u_10m:ms;\n",
      "  optional float field_id=-1 wind_speed_v_10m:ms;\n",
      "  optional float field_id=-1 wind_speed_w_1000hPa:ms;\n",
      "}\n",
      "\n",
      "\n",
      "Schema for file B:\n",
      "<pyarrow._parquet.ParquetSchema object at 0x14627c0c0>\n",
      "required group field_id=-1 schema {\n",
      "  optional int64 field_id=-1 date_forecast (Timestamp(isAdjustedToUTC=false, timeUnit=microseconds, is_from_converted_type=false, force_set_converted_type=false));\n",
      "  optional float field_id=-1 absolute_humidity_2m:gm3;\n",
      "  optional float field_id=-1 air_density_2m:kgm3;\n",
      "  optional float field_id=-1 ceiling_height_agl:m;\n",
      "  optional float field_id=-1 clear_sky_energy_1h:J;\n",
      "  optional float field_id=-1 clear_sky_rad:W;\n",
      "  optional float field_id=-1 cloud_base_agl:m;\n",
      "  optional float field_id=-1 dew_or_rime:idx;\n",
      "  optional float field_id=-1 dew_point_2m:K;\n",
      "  optional float field_id=-1 diffuse_rad:W;\n",
      "  optional float field_id=-1 diffuse_rad_1h:J;\n",
      "  optional float field_id=-1 direct_rad:W;\n",
      "  optional float field_id=-1 direct_rad_1h:J;\n",
      "  optional float field_id=-1 effective_cloud_cover:p;\n",
      "  optional float field_id=-1 elevation:m;\n",
      "  optional float field_id=-1 fresh_snow_12h:cm;\n",
      "  optional float field_id=-1 fresh_snow_1h:cm;\n",
      "  optional float field_id=-1 fresh_snow_24h:cm;\n",
      "  optional float field_id=-1 fresh_snow_3h:cm;\n",
      "  optional float field_id=-1 fresh_snow_6h:cm;\n",
      "  optional float field_id=-1 is_day:idx;\n",
      "  optional float field_id=-1 is_in_shadow:idx;\n",
      "  optional float field_id=-1 msl_pressure:hPa;\n",
      "  optional float field_id=-1 precip_5min:mm;\n",
      "  optional float field_id=-1 precip_type_5min:idx;\n",
      "  optional float field_id=-1 pressure_100m:hPa;\n",
      "  optional float field_id=-1 pressure_50m:hPa;\n",
      "  optional float field_id=-1 prob_rime:p;\n",
      "  optional float field_id=-1 rain_water:kgm2;\n",
      "  optional float field_id=-1 relative_humidity_1000hPa:p;\n",
      "  optional float field_id=-1 sfc_pressure:hPa;\n",
      "  optional float field_id=-1 snow_density:kgm3;\n",
      "  optional float field_id=-1 snow_depth:cm;\n",
      "  optional float field_id=-1 snow_drift:idx;\n",
      "  optional float field_id=-1 snow_melt_10min:mm;\n",
      "  optional float field_id=-1 snow_water:kgm2;\n",
      "  optional float field_id=-1 sun_azimuth:d;\n",
      "  optional float field_id=-1 sun_elevation:d;\n",
      "  optional float field_id=-1 super_cooled_liquid_water:kgm2;\n",
      "  optional float field_id=-1 t_1000hPa:K;\n",
      "  optional float field_id=-1 total_cloud_cover:p;\n",
      "  optional float field_id=-1 visibility:m;\n",
      "  optional float field_id=-1 wind_speed_10m:ms;\n",
      "  optional float field_id=-1 wind_speed_u_10m:ms;\n",
      "  optional float field_id=-1 wind_speed_v_10m:ms;\n",
      "  optional float field_id=-1 wind_speed_w_1000hPa:ms;\n",
      "}\n",
      "\n",
      "\n",
      "Schema for file C:\n",
      "<pyarrow._parquet.ParquetSchema object at 0x146098140>\n",
      "required group field_id=-1 schema {\n",
      "  optional int64 field_id=-1 date_forecast (Timestamp(isAdjustedToUTC=false, timeUnit=microseconds, is_from_converted_type=false, force_set_converted_type=false));\n",
      "  optional float field_id=-1 absolute_humidity_2m:gm3;\n",
      "  optional float field_id=-1 air_density_2m:kgm3;\n",
      "  optional float field_id=-1 ceiling_height_agl:m;\n",
      "  optional float field_id=-1 clear_sky_energy_1h:J;\n",
      "  optional float field_id=-1 clear_sky_rad:W;\n",
      "  optional float field_id=-1 cloud_base_agl:m;\n",
      "  optional float field_id=-1 dew_or_rime:idx;\n",
      "  optional float field_id=-1 dew_point_2m:K;\n",
      "  optional float field_id=-1 diffuse_rad:W;\n",
      "  optional float field_id=-1 diffuse_rad_1h:J;\n",
      "  optional float field_id=-1 direct_rad:W;\n",
      "  optional float field_id=-1 direct_rad_1h:J;\n",
      "  optional float field_id=-1 effective_cloud_cover:p;\n",
      "  optional float field_id=-1 elevation:m;\n",
      "  optional float field_id=-1 fresh_snow_12h:cm;\n",
      "  optional float field_id=-1 fresh_snow_1h:cm;\n",
      "  optional float field_id=-1 fresh_snow_24h:cm;\n",
      "  optional float field_id=-1 fresh_snow_3h:cm;\n",
      "  optional float field_id=-1 fresh_snow_6h:cm;\n",
      "  optional float field_id=-1 is_day:idx;\n",
      "  optional float field_id=-1 is_in_shadow:idx;\n",
      "  optional float field_id=-1 msl_pressure:hPa;\n",
      "  optional float field_id=-1 precip_5min:mm;\n",
      "  optional float field_id=-1 precip_type_5min:idx;\n",
      "  optional float field_id=-1 pressure_100m:hPa;\n",
      "  optional float field_id=-1 pressure_50m:hPa;\n",
      "  optional float field_id=-1 prob_rime:p;\n",
      "  optional float field_id=-1 rain_water:kgm2;\n",
      "  optional float field_id=-1 relative_humidity_1000hPa:p;\n",
      "  optional float field_id=-1 sfc_pressure:hPa;\n",
      "  optional float field_id=-1 snow_density:kgm3;\n",
      "  optional float field_id=-1 snow_depth:cm;\n",
      "  optional float field_id=-1 snow_drift:idx;\n",
      "  optional float field_id=-1 snow_melt_10min:mm;\n",
      "  optional float field_id=-1 snow_water:kgm2;\n",
      "  optional float field_id=-1 sun_azimuth:d;\n",
      "  optional float field_id=-1 sun_elevation:d;\n",
      "  optional float field_id=-1 super_cooled_liquid_water:kgm2;\n",
      "  optional float field_id=-1 t_1000hPa:K;\n",
      "  optional float field_id=-1 total_cloud_cover:p;\n",
      "  optional float field_id=-1 visibility:m;\n",
      "  optional float field_id=-1 wind_speed_10m:ms;\n",
      "  optional float field_id=-1 wind_speed_u_10m:ms;\n",
      "  optional float field_id=-1 wind_speed_v_10m:ms;\n",
      "  optional float field_id=-1 wind_speed_w_1000hPa:ms;\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the metadata of the parquet files\n",
    "metadata_a = pq.read_metadata('A/X_train_observed.parquet')\n",
    "metadata_b = pq.read_metadata('B/X_train_observed.parquet')\n",
    "metadata_c = pq.read_metadata('C/X_train_observed.parquet')\n",
    "\n",
    "get_file_path\n",
    "\n",
    "# Get the schema of the parquet files\n",
    "schema_a = metadata_a.schema\n",
    "schema_b = metadata_b.schema\n",
    "schema_c = metadata_c.schema\n",
    "\n",
    "print(\"Schema for file A:\")\n",
    "print(schema_a)\n",
    "print(\"\\nSchema for file B:\")\n",
    "print(schema_b)\n",
    "print(\"\\nSchema for file C:\")\n",
    "print(schema_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1749    1.0\n",
       "1750    1.0\n",
       "1751    1.0\n",
       "1752    1.0\n",
       "1753    1.0\n",
       "1754    1.0\n",
       "1755    1.0\n",
       "Name: snow_drift:idx, dtype: float32"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "df_A = pq.read_table('A/X_train_observed.parquet').to_pandas()\n",
    "df_B = pq.read_table('B/X_train_observed.parquet').to_pandas()\n",
    "df_C = pq.read_table('C/X_train_observed.parquet').to_pandas()\n",
    "\n",
    "# Inspect the data\n",
    "#print(df_A.info())\n",
    "#print(df_B.info())\n",
    "#print(df_C.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for general feature processing\n",
    "class FeatureProcessingClass():\n",
    "    def __init__(self):\n",
    "        self.categorical_features = ['dew_or_rime:idx',\n",
    "                                    'is_day:idx', \n",
    "                                    'is_in_shadow:idx', \n",
    "                                    'precip_type_5min:idx', \n",
    "                                    'snow_drift:idx']\n",
    "\n",
    "\n",
    "    ###--- METHODS FOR JOINING DATASETS ---###\n",
    "\n",
    "\n",
    "    def create_new_column_names(self, df: pd.DataFrame, suffix: str, columns_no_change: list[str]):\n",
    "        '''\n",
    "        Change column names by given suffix, keep columns_no_change, and return back the data\n",
    "\n",
    "        PARAMS:\n",
    "        - df: data\n",
    "        - suffix: suffixes to add to column names\n",
    "        - columns_no_change: list of column names who should not be changed\n",
    "        '''\n",
    "        df.columns = [col + suffix \n",
    "                      if col not in columns_no_change\n",
    "                      else col\n",
    "                      for col in df.columns\n",
    "                      ]\n",
    "        return df\n",
    "    \n",
    "\n",
    "    ###--- METHODS FOR CREATING/MODIFYING FEATURES ---###\n",
    "\n",
    "\n",
    "    def impute_missing_values(self, df: pd.DataFrame):\n",
    "        '''\n",
    "        Impute missing values with mean of the two closest non-NaN values\n",
    "\n",
    "        PARAMS:\n",
    "        - df: pandas data\n",
    "        '''\n",
    "        # Create a KNNImputer\n",
    "        imputer = KNNImputer(n_neighbors=2)\n",
    "\n",
    "        # Fit and transform the DataFrame\n",
    "        df_imputed = imputer.fit_transform(df)\n",
    "        df_imputed = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "\n",
    "        return df_imputed\n",
    "\n",
    "    \n",
    "    def create_time_features(self, df: pd.DataFrame):\n",
    "        '''\n",
    "        Create data features based on datetime column\n",
    "\n",
    "        PARAMS:\n",
    "        - df: data\n",
    "        '''\n",
    "\n",
    "        # Check if 'date_forecast' column exists\n",
    "        if 'date_forecast' not in df.columns:\n",
    "            raise ValueError(\"DataFrame does not have 'date_forecast' column\")\n",
    "\n",
    "        # Try to convert 'date_forecast' to datetime\n",
    "        try:\n",
    "            df['date_forecast'] = pd.to_datetime(df['date_forecast'])\n",
    "        except Exception as e:\n",
    "            raise ValueError(\"Cannot convert 'date_forecast' to datetime: \" + str(e))\n",
    "\n",
    "        # time period features\n",
    "        df['date'] = df['date_forecast'].dt.normalize()\n",
    "        df['year'] = df['date_forecast'].dt.year\n",
    "        df['quarter'] = df['date_forecast'].dt.quarter\n",
    "        df['month'] = df['date_forecast'].dt.month\n",
    "        df['week'] = df['date_forecast'].dt.isocalendar().week\n",
    "        df['hour'] = df['date_forecast'].dt.hour\n",
    "\n",
    "        # day features\n",
    "        df['day_of_year'] = df['date_forecast'].dt.day_of_year\n",
    "        df['day'] = df['date_forecast'].dt.day\n",
    "        df['weekday'] = df['date_forecast'].dt.weekday\n",
    "\n",
    "        # boolean features\n",
    "        df['is_weekend'] = df['weekday'] > 5\n",
    "\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def convert_to_categorical(self, df: pd.DataFrame, categorical_features: list[str]):\n",
    "        '''\n",
    "        Convert columns to categorical dtype\n",
    "\n",
    "        PARAMS:\n",
    "        - df: data\n",
    "        - catagorical_features: list of columns to convert to categorical\n",
    "        '''\n",
    "        return  df[categorical_features].apply(lambda x: x.astype('category'))\n",
    "    \n",
    "\n",
    "    def convert_to_numerical(self, df: pd.DataFrame, numerical_features: list[str]):\n",
    "        '''\n",
    "        Convert columns to numerical dtype\n",
    "        \n",
    "        PARAMS:\n",
    "        - df: data\n",
    "        - numerical_features: list of columns to convert to numerical\n",
    "        '''\n",
    "        return  df[numerical_features].apply(lambda x: x.astype('float'))\n",
    "    \n",
    "\n",
    "    def create_lag_features(self, df: pd.DataFrame, lag_features: list[str], lag_values: list[int]):\n",
    "        '''\n",
    "        Create lag features for given columns\n",
    "\n",
    "        PARAMS:\n",
    "        - df: data\n",
    "        - lag_features: list of columns to create lag features for\n",
    "        - lag_values: list of lag values\n",
    "        '''\n",
    "        # checking that not any lag features is categorical\n",
    "        if any(feature in lag_features for feature in self.categorical_features):\n",
    "            catagorical_feature_list = [feature for feature in lag_features if feature in self.categorical_features]\n",
    "            raise ValueError(f\"Cannot create lag features for categorical features {catagorical_feature_list}\")\n",
    "\n",
    "        for feature in lag_features:\n",
    "            for lag in lag_values:\n",
    "                df[feature + '_lag_' + str(lag)] = df[feature].shift(lag)\n",
    "        return df\n",
    "    \n",
    "\n",
    "    ###--- CALL METHOD FOR INITIALISING CLASS AS FUNCTION ---###\n",
    "\n",
    "\n",
    "    def __call__(self, data, client, historical_weather, forecast_weather, electricity, gas):\n",
    "        '''Processing of features from all datasets, merge together and return features for dataframe df '''\n",
    "        # Create features for relevant dataset\n",
    "        data = self.create_data_features(data)\n",
    "        client = self.create_client_features(client)\n",
    "        historical_weather = self.create_historical_weather_features(historical_weather)\n",
    "        forecast_weather = self.create_forecast_weather_features(forecast_weather)\n",
    "        electricity = self.create_electricity_features(electricity)\n",
    "        gas = self.create_gas_features(gas)\n",
    "        \n",
    "        # 🔗 Merge all datasets into one df 🔗\n",
    "        df = data.merge(client, how='left', on = self.client_join)\n",
    "        df = df.merge(historical_weather, how='left', on = self.weather_join)\n",
    "        df = df.merge(forecast_weather, how='left', on = self.weather_join)\n",
    "        df = df.merge(electricity, how='left', on = self.electricity_join)\n",
    "        df = df.merge(gas, how='left', on = self.gas_join)\n",
    "        \n",
    "        # Change columns to categorical for XGBoost\n",
    "        df[self.category_columns] = df[self.category_columns].astype('category')\n",
    "        return df\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TDT4173_Machine_Learning_EnergyOutput)",
   "language": "python",
   "name": "tdt4173_machine_learning_energyoutput"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
